services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:5000
      - NEXT_PUBLIC_WS_URL=ws://localhost:5000
    depends_on:
      - backend
    networks:
      - marie-network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    ports:
      - "5000:5000"
    volumes:
      - ./backend:/app
      - ./backend/uploads:/app/uploads
      - hf_cache:/root/.cache/huggingface
    environment:
      - FLASK_ENV=development
      - OPENSEARCH_HOSTS=${OPENSEARCH_HOSTS:-http://localhost:9200}
      - OPENSEARCH_USER=${OPENSEARCH_USER:-admin}
      - OPENSEARCH_PASSWORD=${OPENSEARCH_PASSWORD:-}
      - OPENSEARCH_USE_SSL=${OPENSEARCH_USE_SSL:-false}
      - OPENSEARCH_VERIFY_CERTS=${OPENSEARCH_VERIFY_CERTS:-false}
      - JWT_SECRET_KEY=dev-secret-key-change-in-production
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://localhost:11434}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY:-}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on: []
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - marie-network

volumes:
  hf_cache:
    driver: local

networks:
  marie-network:
    driver: bridge
